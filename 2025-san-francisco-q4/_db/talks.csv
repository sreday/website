YouTube,status,name,track,day,organization,photo,linkedin,linkedin2,twitter,twitter2,title,abstract,description,bio
,keynote,Matt Schillerstrom,1,1,Harness,Matt Schillerstrom.png,https://www.linkedin.com/in/matt-schillerstrom-722b5559/,,,,Keynote: TBD,,,
,keynote,Gian Merlino,1,1,Imply,Gian Merlino.png,https://www.linkedin.com/in/gianmerlino/,,,,"Keynote: What Observability Can Learn From BI: Decoupling for Speed, Scale, and Flexibility","Today’s observability platforms are often vertically integrated—binding data storage, query, and visualization layers into a single stack. This tight coupling drives up costs, makes integrations painful, and slows teams down. But it doesn’t have to be this way.
In this talk, we’ll explore how SRE teams can benefit from a more modular approach to observability—one inspired by the evolution of Business Intelligence. Just as BI stacks evolved to separate ETL, data warehouses, and dashboards, observability stacks can be designed around clear boundaries: interoperable tools, technology-neutral query layers, and plug-and-play storage.
You’ll learn why decoupled observability architecture is essential for cost control, agility, and tool flexibility—and how to move toward a stack that meets the real-world needs of today’s SRE teams.",,"Gian is one of the original authors of the open source Apache Druid® project and co-founder and CTO at Imply. Gian served as the Apache Druid project’s first PMC chair. Previously, Gian led the data ingestion team at Metamarkets and held senior engineering positions at Yahoo. He holds a B.S. in Computer Science from Caltech. "
,confirmed,Steve Poyer,1,1,CAST AI,Steve Poyer.png,https://www.linkedin.com/in/stevepoyer/,,,,Container Live Migration in Kubernetes: Why and How,"What if Kubernetes could move a running container to another node without a single second of downtime? In this session, we’ll dive into Container Live Migration—a game-changing capability that brings a new level of availability to your workloads. Say goodbye to disruption from Spot instance terminations, node pressure, or scaling events—even for apps with long startup times like Java services, long-running jobs, or stateful workloads.

We’ll explore how live migration works under the hood, why it’s a powerful cost-saving tool for intelligent bin-packing, and show it all in action with a live demo.",,"Steve Poyer is an experienced pre-sales systems engineer with a strong background in cloud-native infrastructure, Kubernetes orchestration, and container runtime optimization. He has worked in both enterprise and startup environments, delivering high-availability, low-latency solutions for mission-critical workloads on AWS, Azure, and GCP.

In his current role at Cast AI, Steve advises organizations on optimizing Kubernetes performance and cost efficiency through advanced scheduling, autoscaling, and workload placement strategies. With hands-on expertise in cluster design, node management, and storage/network integration, Steve brings a unique mix of technical depth and practical SRE know-how to the topic of Container Live Migration."
,confirmed,Aditya Bansal,1,1,Cortex,Aditya Bansal.png,https://www.linkedin.com/in/adbansal/,,,,“We’re Down!” to “We’re Good.” — Shipping observability in 2 weeks,"**Many hyper-growth startups hit a point where the current systems just aren’t enough.**  
Racing toward product–market fit, they skip best practices around observability, monitoring, and alerting—and pay for it later.

This talk is about going from **0 → 1** and protecting your company, team, and customers when the pressure mounts.

**I’ll cover:**

- **Getting started from the ground floor**  
  Foundational work, the three pillars of observability, and—more importantly—how to get hands-on with all three.  
  This won’t be another high-level “logs/traces/metrics” sermon; we’ll actually use them.

- **Building your first monitor, alert, and dashboard**  
  Move to offense: catch full outages, errors, latency spikes, and change alerts *before* your customers do.  
  We’ll touch more than just the “golden signals.”

- **Iterating**  
  Going from 0 → 1 is only the beginning. You’ll need to tune false alarms, coach engineers on response, add new metrics, and prune stale ones—plenty of hidden gotchas here.

Teams often get bogged down by dogma and decision paralysis.  
I’ll share tactics for keeping a **bias toward action** and steadily moving the reliability needle.

*No sales pitch.* I may demo with an APM tool (no affiliations) purely to illustrate what’s possible.
",,"Aditya Bansal is a Staff Engineer at Cortex, an Internal Developer Portal that helps engineering teams catalog their services. He joined as the company’s first employee over four years ago and has since grown into his current Staff Engineer role.

Aditya began his career at Poynt, helping scale the engineering team from 15 to 60+ people. There, he built and maintained much of the company-wide infrastructure and watched the platform make the classic transition from a monolith to microservices.

He later joined Curebase as one of the earliest hires, working directly with the founders to build the engineering organisation from scratch before moving on to Cortex."
,confirmed,Deepika Annam,1,1,Nike,Deepika Annam.png,https://www.linkedin.com/in/deepika-annam-953868223/,,,,Building Self-Healing Data Pipelines: How Reinforcement Learning Reduces Operational Overhead While Improving Performance,"Site Reliability Engineers face escalating challenges managing data pipelines that must adapt to dynamic workloads, handle traffic spikes, and maintain high availability across distributed cloud environments. Traditional static optimization and rule-based approaches fall short when dealing with the complexity of modern streaming systems, often requiring extensive manual intervention and creating reliability risks during peak demand periods. This presentation demonstrates how Deep Reinforcement Learning (DRL) is revolutionizing pipeline reliability engineering, enabling truly self-healing systems that automatically optimize performance while reducing operational burden. Through analysis of production deployments and research findings, we'll explore how DRL-powered systems consistently outperform conventional approaches across critical reliability metrics. Research demonstrates significant operational improvements: Apache Spark implementations using RL-based resource allocation show measurable performance gains compared to heuristic policies while achieving better resource efficiency. Apache Flink deployments with RL-based flow control demonstrate substantial latency reductions and throughput increases compared to rule-based systems. Modern lightweight DRL architectures can achieve significant improvements in data ingestion throughput while converging to optimal solutions rapidly on complex production pipelines. The operational benefits extend beyond immediate performance gains. With maintenance activities consuming substantial portions of production costs, DRL-based predictive maintenance has seen significant growth in research adoption. Data center implementations show meaningful power savings in thermal management scenarios, while advanced algorithms demonstrate high success rates in automated remediation applications. These results highlight DRL's potential for reducing operational overhead while improving system reliability. Attendees will learn practical implementation strategies for neural network integration in production environments, adaptive resource allocation patterns for cloud-native systems, and multi-objective optimization frameworks balancing performance, cost, and reliability. We'll cover real-world applications using Deep Q-Networks, Proximal Policy Optimization, and Soft Actor-Critic algorithms across various industries. The session includes implementation frameworks, reliability benchmarking methodologies, and strategies for addressing operational challenges like hyperparameter tuning and model deployment, providing actionable insights for building next-generation self-managing pipeline systems that reduce manual intervention, improve system resilience, and enable proactive operational practices.",,"Deepika Annam is a Senior Data Engineer at Nike Inc with over 14 years of experience in data engineering, software development, and SAP consulting. Currently based in Portland, OR, she specializes in architecting scalable data solutions and developing robust data pipelines that support business intelligence and reporting across financial, inventory, sales, and planning domains. At Nike, Deepika has led major platform transformations, including the migration of approximately 1,500 SAP BusinessObjects reports to 250+ Power BI reports and the upgrade from EMR to Databricks, achieving significant cost savings of approximately 80%. Her work on real-time dashboards and data solutions has directly contributed to business growth, including a 15% increase in North America Thanksgiving sales. Deepika holds an MBA from Andhra University and a Bachelor's degree in Engineering from SRKR Engineering College, India. She is skilled in SQL, Python, PySpark, AWS, Airflow, and various SAP technologies, with additional certifications in data analysis and Python for data engineering from Great Learning Academy and Duke University. Her expertise spans modern data warehouse solutions, ETL pipeline development, and business intelligence reporting, making her a valuable asset in driving data-driven decision making for enterprise-level organizations."
,confirmed,Ruchir Jha,1,1,CardinalHQ,Ruchir Jha.png,https://www.linkedin.com/in/ruchirpjha/,,,,Context Engineering in Observability: The next SRE Superpower,"As AI systems grow more capable, their usefulness hinges not just on what they know, but what context they understand. In this talk, Ruchir Jha, CEO of Cardinal — an AI-powered observability company, unpacks the emerging discipline of context engineering: the art and science of feeding AI the right information, at the right time, to make sense of complex, high-dimensional infrastructure systems.

Drawing on real-world use cases from cloud-native observability, the talk explores how AI can go beyond dashboards and alerts to reason, adapt, and troubleshoot like an SRE. We’ll explore lessons from building systems that turn noisy telemetry into structured insight, and how context-aware AI is redefining reliability, performance, and cost-efficiency in modern infrastructure.

Whether you're working on AI models, backend systems, or distributed ops, this talk will give you a new lens on how to make machines think contextually and why that's the next frontier in AI for infrastructure.",,"I am the CEO & Co-Founder of [Cardinal](cardinalhq.io), an AI-native observability platform built for modern infrastructure. Before this, I spent seven years at Netflix leading observability engineering including building petabyte-scale systems used daily by thousands of engineers. At Cardinal, I’m focused on pioneering context-aware observability, so ops teams can move beyond dashboards and alerts, and toward intelligent systems that think and reason. I have spoken at SREday San Francisco before, on topics like cost-neutral cardinality in observability, and appeared on podcasts discussing how AI can reduce spiraling observability costs through strategic instrumentation and tooling choices."
,confirmed,Avi Press,1,1,Scarf,Avi Press.png,https://www.linkedin.com/in/avi-press-4437a356/,,,,10 Billion Downloads: Insights and Trends in Open Source,"In this talk, we share the up-to-date results and fresh insights of an in-depth analysis of data gathered from over 10 billion events analyzed across thousands of projects. The analysis reveals a clearer view of the latest emerging trends.

Our findings offer valuable insights into user behaviors and interactions with open source software, making it essential for maintainers, founders, and executives in open source companies. We will delve deep into our data, uncovering the best practices employed by successful open source projects.

We’ll explore critical trends, including packaging formats, geographic download shifts, and the role of documentation in user retention. Additionally, we’ll examine how community engagement impacts project success and what maintainers can do to drive growth.

Attendees can expect to leave this talk equipped with actionable insights and best practices to optimize their open source projects and thrive in the competitive landscape of open source software.",,"Avi Press is a developer tool author, and functional programming language enthusiast, serving as a founder & CEO at Scarf. Avi loves thinking about and discussing how people can solve problems by more effectively sharing data, and how that applies to building a sustainable open-source ecosystem. He is also a host of the Hacking Open Source Business Podcast, where Avi continues to share insights, inspire discussions, and contribute to the ever-evolving open-source landscape."
,confirmed,Saurabh Kumar & Ruskin Dantra,1,1,AWS,Saurabh Kumar & Ruskin Dantra.png,https://www.linkedin.com/in/saurabh-kumar-sk/,https://www.linkedin.com/in/ruskindantra/,,,Transform chaos experiments into actionable insights using generative AI,"Tired of manual chaos experiment analysis? Discover how to leverage generative AI to analyze test results and validate experiment hypothesis. Learn to integrate Amazon Bedrock with AWS FIS to transform your chaos engineering experiments and game days into efficient, data-driven exercises that prove system resilience.",,"**Saurabh Kumar** is an experienced technology leader with over two decades in the software and cloud architecture domain. He currently serves as a Sr. Solutions Architect at Amazon, where he has been driving strategic cloud initiatives since 2022. Prior to Amazon, Saurabh held the role of Principal Architect at Fidelity Investments for over 15 years, leading complex enterprise solutions and architecture transformations. He began his career as a Senior Software Engineer at Patni Computer Systems, gaining a strong foundation in software development. Based in Raleigh, North Carolina, Saurabh brings deep expertise in cloud architecture, enterprise systems, and Failure Mode and Effects Analysis (FMEA).

**Ruskin Dantra** is a seasoned Solutions Architect with a strong track record of delivering scalable and innovative software solutions across diverse industries. Currently based in the San Francisco Bay Area, he has been with Amazon Web Services (AWS) since 2021, first in Auckland, New Zealand, and now in a hybrid role in the U.S., focusing on helping organizations accelerate their cloud journeys. With over 15 years of experience, Ruskin has held technical leadership roles including Principal Engineer and Chapter Lead at ASB Bank, and Development Team Lead at Nintex, where he led software architecture and engineering efforts. He is known for his pragmatic approach to system design, mentoring teams, and driving best practices in software development. Passionate about animal welfare, he also volunteered for five years at SPCA Auckland, supporting dog adoption and care."
,confirmed,Vlad Seliverstov,1,1,ClickHouse,Vlad Seliverstov.png,https://www.linkedin.com/in/behemot/,,,,Beyond 100 Petabytes: Why We Built a Custom Exporter to Replace Our OTel Pipeline,"Are your observability signals trapped in separate pillars? Logs in one place, metrics in another, both losing context? At ClickHouse, we faced this challenge at a massive scale. Our solution was to abandon the traditional model and embrace a new philosophy: store everything, aggregate nothing. This talk charts our journey to 100 PB and 500 trillion rows, centered on the concept of “wide events.” Instead of shipping a simple log message and a separate, pre-aggregated metric, we store a single, context-rich event containing every possible dimension. This shift from three pillars to a single warehouse of high-cardinality telemetry was a game-changer. The key to this model is using ClickHouse itself as our observability backend. This unlocks unbounded query flexibility through full SQL. When an engineer asks “what’s the p95 pod replacement time after termination?”, we don’t say “let me ship a new metric.” We write a SQL query.",,"Vlad Seliverstov leads the internal observability team at ClickHouse, overseeing the monitoring of ClickHouse Cloud, which handles over 100 petabytes of data and 500 trillion events. With more than a decade of SRE experience at Datadog, Dropbox, and Facebook, Vlad now focuses on building and scaling ClickHouse’s in-house observability platform."
,confirmed,Dwayne McDaniel,1,1,GitGuardian,Dwayne McDaniel.png,https://www.linkedin.com/in/dwaynemcdaniel/,,,,Secrets Security End-To-End,"Credentials allow human-to-machine and machine-to-machine communication. According to CyberArk's recent research, 93% of organizations had two or more identity-related breaches in the past year. It is clear that we need to address this growing issue. Unfortunately, many organizations are OK with using plaintext credentials, which we should all know not to do by now. Given the scope of the problem, what can we do? Let's make a plan! Secrets Detection Secrets Management Developer Workflows Secrets Scanning Automatic Rotation

By the end of this session, you should have a clear roadmap for taming the machine identity mess in your code and pipelines. `",,"Dwayne has been working as a Developer Advocate since 2014 and has been involved in tech communities since 2005. His entire mission is to “help people figure stuff out.” He loves sharing his knowledge, and he has done so by giving talks at hundreds of events worldwide. He has been fortunate enough to speak at institutions like MIT and Stanford and internationally in Paris and Iceland. Dwayne currently lives in Chicago. Outside of tech, he loves karaoke, live music, and crochet."
,confirmed,Piyush Dubey,2,1,Microsoft,Piyush Dubey.png,https://www.linkedin.com/in/piyushdubey07/,,,,Data Lakehouse Architecture: Reducing Operational Complexity for SRE Teams,"Modern enterprise data infrastructure creates significant operational overhead for SRE teams, with organizations spending the majority of their engineering cycles managing ETL pipelines, data replication, and maintaining multiple storage systems across data warehouses, lakes, and specialized databases. This operational complexity directly impacts service reliability, increases mean time to recovery (MTTR), and creates numerous points of failure that challenge SLO achievement. This presentation demonstrates how data lakehouse architecture transforms operational paradigms by consolidating disparate data systems into unified, cloud-native platforms that dramatically reduce infrastructure complexity. Through practical implementation strategies, we explore how SRE teams can eliminate redundant data copies, reduce operational toil through automated governance frameworks, and achieve better observability through centralized metadata management. Key operational benefits include substantial reduction in data pipeline maintenance overhead, simplified monitoring through unified observability stacks, and improved incident response through consolidated failure domains. The architecture leverages open table formats like Delta Lake and Apache Iceberg to ensure vendor neutrality while enabling automated backup, recovery, and compliance workflows. We examine five critical architectural layers from an operational perspective: scalable cloud storage foundations with built-in redundancy, self-healing metadata catalogs that reduce manual intervention, automated semantic layers that abstract operational complexity, and optimized query engines that provide consistent performance under varying loads. Attendees will learn practical migration strategies for reducing operational complexity, frameworks for measuring reliability improvements through SLI/SLO metrics, and automation patterns for minimizing manual operational overhead. This session provides actionable insights for SRE teams seeking to modernize data infrastructure while improving service reliability and reducing operational burden.",,"Piyush Dubey is a technology professional with over a decade of experience architecting and developing large-scale, distributed systems in cloud-native environments. Currently serving as a Senior Software Engineer at Microsoft since March 2021, he specializes in building high-performance data platforms, scalable microservices, and lakehouse solutions using technologies including Apache Spark, Delta Lake, Apache Iceberg, and Hive. At Microsoft, Piyush leads the Metadata Interoperability initiative to support open table formats in OneLake, enabling seamless integration with external data platforms like Snowflake. He designed and developed the Table Preview API to enable data governance and sensitivity labeling across OneLake datasets, supporting GDPR and CCPA compliance. As part of the founding team for OneSecurity, he built the OneLake Data Access Roles framework to enable fine-grained access on data artifacts. Prior to Microsoft, Piyush worked at Amazon on the Display Advertising team developing APIs for ad delivery and publishing across Kindle devices, and contributed to data compliance APIs supporting GDPR and CCPA requirements. At ServiceNow, he designed and implemented a scalable onboarding automation workflow engine that became a full-fledged product marketed as Employee Workflows, and led the design of a web-based PDF editor that digitized signature workflows. During his time at Pearson, he led the redesign and migration of a legacy SOAP-based address verification service to a RESTful API, resulting in a 65% performance improvement. Piyush holds a Master's degree in Computer Science from the University of Iowa and a Bachelor's degree in Information Technology from Rajiv Gandhi Technical University. He holds a patent for Sound Assessment and Remediation and has expertise across multiple programming languages and cloud platforms including Azure, AWS, and Google Cloud. He is based in Boston, MA."
,confirmed,Jimmy Katiyar,2,1,SiriusXM Radio,Jimmy Katiyar.png,https://www.linkedin.com/in/jimmykatiyar/,,,,The Human Factor in Site Reliability: Designing Automation That Amplifies Engineering,"As automation sophistication increases across SRE practices, organizations face a critical inflection point: whether to pursue lights-out operations or embrace human-centered reliability engineering that delivers measurably superior outcomes. This presentation reveals how leading tech organizations achieve significantly higher system reliability and faster incident resolution by strategically designing automation as engineering amplification rather than replacement. Based on comprehensive analysis of incident response patterns and drawing from my experience implementing AI-powered operational workflows at SiriusXM, this session demonstrates how organizations adopting human-centered SRE practices achieve faster mean time to resolution (MTTR) and fewer repeat incidents compared to fully automated approaches. Real-world case studies examine intelligent alerting systems that provide contextual incident classification while preserving human expertise for complex failure mode analysis, and automated remediation tools that execute standard procedures while escalating edge cases requiring engineering judgment. The presentation addresses critical operational challenges including alert fatigue, over-automation risks that mask system complexity, and cognitive load management for on-call engineers. Attendees discover proven frameworks for designing multi-tiered incident response that achieves substantial efficiency gains while maintaining engineering situational awareness, and learn how strategic automation reduces toil while enhancing rather than replacing critical thinking skills. Key implementation strategies include developing runbook automation that educates while executing, establishing escalation frameworks that improve incident learning outcomes, and creating measurement systems that capture both operational metrics and engineer wellness indicators. Organizations following these human-centered reliability principles consistently achieve better uptime, faster recovery, and higher team satisfaction compared to automation-first approaches. This session provides SRE leaders with practical tools for building resilient operations that leverage automation strategically while preserving the engineering judgment essential for managing complex distributed systems.",,"Jimmy Katiyar is a Senior Manager of Product Management at SiriusXM Inc., where he leads Salesforce-based ad sales technology transformations across media, satellite, streaming, and podcast platforms. As a Certified Scrum Product Owner, he serves as both a Product Leader and Functional Solution Architect, supporting cross-functional domains including Sales Development, Marketing, Listener Support, Legal, Programmatic Sales, Ad Creative Services, Finance, Credit & Collections, and Training Enablement. In his current role, Jimmy spearheads critical initiatives for streaming and podcast campaign booking, brand categorization, show monetization, and enforcement of ad block category and brand preferences. He drives unified Quote-to-Cash programs, leading Direct and Programmatic Sales redesign with opportunity product line-item integration, split approval processes, insertion order automation, and reporting data aggregation. His leadership of the Revenue Recognition project resulted in the successful design and implementation of a payment portal processing $750K–$3.5M daily, enabling advertisers and agencies to pay invoices via ACH or credit cards while integrating with Oracle Finance systems. Jimmy's technical expertise spans Salesforce Service Cloud optimization, where he has delivered AI-powered solutions including DeepConverse Chat and Einstein AI features for case classification, article recommendation, case closure, and next best action workflows. His architectural work includes designing omni-channel routing systems, entitlement management for case milestones, CSAT automation, and creating unified ""single pane of glass"" interfaces for service agents. He also architected Salesforce processes for legal digital signature workflows using Adobe Sign for NDAs. With over two decades of experience in enterprise technology solutions, Jimmy has held progressive leadership roles at organizations including Equinix Inc., IBM Global Business Services, Hewlett Packard, and Tech Mahindra. His international experience includes traveling to the UK and Singapore to support regional product integration efforts, aligning global teams and system designs for scalable, maintainable solutions tailored to global support operations. Jimmy holds a Bachelor of Technology degree from MJP Rohilkhand University and maintains multiple industry certifications, including Salesforce Certified AI Associate, Service Cloud Consultant, and Administrator credentials, along with Certified Scrum Product Owner from Scrum Alliance, Oracle Siebel CRM Consultant, and IBM Enterprise Design Thinking Practitioner certifications. His expertise in streamlining ad sales, AI strategies, product integration, business process re-engineering, and customer experience continues to drive innovation in ad sales technology transformations."
,confirmed,Ran Tao,2,1,AWS,Ran Tao.png,https://www.linkedin.com/in/tomtaoran/,,,,"Migration from On-Prem Messaging System to The Cloud: What, How and Why","The race to the cloud is on, with enterprises everywhere migrating core infrastructure to stay competitive and cost effective. But when it comes to the messaging systems that power cross-component communications, a simple ""lift and shift"" isn't adequate and can be a recipe for failure. The migration path is riddled with complex decisions and design pitfalls unique to every use case. In this session, AWS Cloud Support expert Tom will walk you through the critical stages of rehosting, replatforming, and refactoring, showing you how to unlock maximum performance and reliability for messaging systems. Additionally, Tom will compare traditional message brokers with more modernized serverless messaging services on AWS. By the end of the session, you will have a much more comprehensive understanding of the migration prcess, key questions to ask and some best practices for harnessing the benefits of Cloud.",,Tom is a Cloud Support Engineer at AWS with five years of dedicated experience. He has gained extensive expertise in cloud-based messaging systems by guiding hundreds of customers through the process of migrating their on-premise systems to the cloud and troubleshooting any issues that arise.
,confirmed,Sureshkumar Karuppuchamy,2,1,eBay,Sureshkumar Karuppuchamy.png,https://www.linkedin.com/in/sureshkumar-karuppuchamy-51282b7/,,,,From Dashboard to Defense: Automating Resilience at Large Scale,"Modern production systems can no longer rely on static dashboards and reactive on-call rotations to ensure uptime. At large scale — with billions of requests flowing through mission-critical services — reliability must be engineered into the system through autonomous detection, mitigation, and recovery.
In this session, I’ll share how our platform team evolved from traditional observability stacks to an integrated, self-defending resilience architecture that transforms metrics into real-time, automated mitigations.
Key topics include:
Actionable observability: Designing high-fidelity Prometheus instrumentation that surfaces actionable SLO breaches and capacity anomalies — not just vanity metrics.
Closed-loop alerting: Building alert pipelines that automatically trigger mitigations, including traffic shaping, circuit breaking, and dynamic configuration changes.
Continuous delivery at scale: How we implemented fully automated CI/CD pipelines with canary deployments, progressive rollouts, and automatic rollback — eliminating manual gates while preserving production stability.
Dynamic rate limiting: Using adaptive throttling to contain abusive or runaway workloads before they impact critical path services.
Proactive incident response: Real-world learnings from production incidents that shaped our automated safeguards, including post-incident automation improvements and resilience patterns.
Operational trust: Governance strategies for enabling engineers to trust self-healing automation, from progressive rollout policies to guardrails for fail-safe operation.
Attendees will gain a practical blueprint for evolving traditional monitoring into an autonomous resilience layer — with concrete patterns, architectural considerations, and lessons learned operating a high-volume, always-on platform.
Whether you’re modernizing your incident response playbooks, tightening your feedback loops, or scaling continuous delivery for critical systems, you’ll leave with actionable strategies to move beyond dashboards — and build a production environment that can defend itself.
Key Takeaways How to evolve from passive observability to automated corrective action. Designing metrics pipelines that detect and trigger real-time mitigations. Safe automation of deployments at scale without sacrificing reliability. Implementing dynamic safeguards like adaptive rate limiting and circuit breaking. Practical leadership and governance approaches for building trust in self-healing systems.",,"Sureshkumar Karuppuchamy is a technology leader with more than two decades of experience designing and modernizing large-scale, AI-enabled infrastructure for some of the world’s most complex platforms. In his current role as a senior engineering leader at eBay, he has led critical modernization efforts across core systems—revamping legacy platforms, transitioning to cloud-native data solutions, and reimagining API architectures to improve agility, reliability, and scalability.
His work includes the development of advanced compliance systems that support real-time moderation and auditing in alignment with global regulations like the EU Digital Services Act. He’s also helped shape seller experience through intuitive listing flows and AI-powered tools that streamline product onboarding, such as transforming product images into fully generated listings.
Sureshkumar’s contributions have been featured in publications including The Guardian, Deloitte WSJ Insights, and marketscreener.com. He began his career at Oracle, building enterprise solutions for global supply chains, and is a graduate of Anna University’s College of Engineering, Guindy.
Passionate about knowledge-sharing, he mentors technologists, contributes to peer-reviewed research, and regularly speaks at international conferences on system architecture, AI, data platforms, and compliance tech."
,confirmed,Gangadharan Venkataraman,2,1,Starbucks,Gangadharan Venkataraman.png,https://www.linkedin.com/in/ganga5v/,,,,Building Bulletproof ML Inference Platforms: SRE Principles for Real-Time AI at Scale,"Real-time machine learning inference platforms present unique SRE challenges that traditional monitoring and reliability practices often can't address. This talk provides a comprehensive framework for applying SRE principles to ML inference systems, drawing from hands-on experience scaling platforms that serve billions of daily predictions with sub-100ms latency requirements. We'll explore how to establish meaningful SLIs and SLOs for ML systems, where traditional availability metrics fall short in capturing model performance degradation, data drift, and inference quality issues. Learn practical approaches to incident response for ML platforms, including automated fallback mechanisms, circuit breakers for model failures, and graceful degradation strategies that maintain user experience during outages. The session covers essential reliability patterns including blue-green deployments for model updates, canary releases with statistical significance testing, and rollback strategies that account for model warming and feature pipeline dependencies. We'll examine monitoring and observability strategies that go beyond traditional metrics, incorporating model performance tracking, feature drift detection, and business impact correlation. Infrastructure reliability techniques will be demonstrated through real-world examples: implementing request batching for throughput optimization while maintaining latency SLAs, designing feature stores for consistency and disaster recovery, and orchestrating Kubernetes-based serving infrastructure with proper resource allocation and auto-scaling policies. Critical operational aspects include capacity planning for ML workloads with variable computational requirements, managing dependencies between feature generation pipelines and serving systems, and implementing effective on-call procedures for ML-specific incidents. Attendees will gain practical tools for building resilient ML inference platforms including monitoring dashboards, alerting strategies, and runbook templates. This session bridges the gap between traditional SRE practices and modern ML operations, providing actionable frameworks for maintaining reliable AI systems that deliver consistent business value while meeting stringent performance requirements.",,"Gangadharan Venkataraman is a highly accomplished technology leader based in Bellevue, Washington, with a strong record of envisioning, architecting, and delivering cutting-edge software and AI/ML solutions for leading global enterprises. With over 18 years of experience across diverse domains including e-commerce, telecommunications, healthcare, and enterprise technology, Gangadharan brings deep technical expertise and strategic leadership to every initiative. Currently serving as Senior Engineer - AI/ML Platform at Starbucks, he leads the design and optimization of large-scale machine learning infrastructure. He recently spearheaded the launch of ML Platform v2.0, drove the Databricks-to-Unity Catalog migration, and championed data governance enhancements, all to improve scalability, performance, and model reliability enterprise-wide. Previously, at UST Global (client: T-Mobile), Gangadharan led key modernization efforts, including Kubernetes-based Gen4 migrations and critical database upgrades for T-Mobile’s Customer Hub platform. Prior to that, he had a transformative 11+ year tenure at eBay, where as a Member of Technical Staff II, Core AI, he led the development of scalable ML infrastructure, real-time personalization pipelines, and video commerce platforms. His contributions powered over 30 billion daily inferences and $200M in annual GMV from event-triggered campaigns. Earlier roles at Computer Science Corporation and GE Healthcare saw him building resilient email platforms and clinical decision-support systems, with a strong focus on automation, compliance, and user-centric design. Gangadharan holds an M.S. in Computer Science from the Georgia Institute of Technology and a B.Tech in Information Technology from the University of Madras. His technical toolkit includes expertise in distributed systems, MLOps, big data pipelines, and cloud-native development using Java, Scala, Python, Kubernetes, and Azure. A recognized leader with a passion for innovation, Gangadharan thrives on solving complex problems, building high-performing teams, and delivering impactful products that scale."
,confirmed,Anjan Dash,2,1,Meta,Anjan Dash.png,https://www.linkedin.com/in/anjan-dash/,,,,Bulletproofing Trillion-Parameter Training: SRE Strategies for Ultra-Large AI Infrastructure at Scale,"Training trillion-parameter language models presents unique site reliability challenges that dwarf traditional distributed systems complexity. With training costs exceeding millions of dollars and runs spanning months across thousands of GPUs, even minor infrastructure failures can result in catastrophic business impact and resource waste. This presentation examines the SRE principles and practices essential for maintaining high uptime in distributed AI training environments. Drawing from real-world experience managing production AI infrastructure, the session explores how frameworks like NVIDIA's Megatron-LM, Microsoft's DeepSpeed, and UC Berkeley's Alpa introduce novel reliability challenges that traditional monitoring and alerting systems cannot adequately address. Key topics include implementing robust fault tolerance mechanisms for multi-week training jobs, designing effective checkpointing strategies that balance recovery speed with storage costs, and building comprehensive observability pipelines for distributed training workloads. The presentation examines how communication overhead between thousands of nodes creates cascading failure scenarios and demonstrates monitoring techniques for detecting performance degradation before it impacts model convergence. The session covers practical SRE implementations including automated failure recovery systems, capacity planning for massive memory requirements, and energy-aware resource allocation strategies. The discussion addresses the operational complexity of managing heterogeneous GPU clusters and the monitoring strategies needed to track model performance alongside traditional infrastructure metrics. Attendees will learn actionable SRE practices for large-scale AI infrastructure including incident response procedures for distributed training failures, reliability testing methodologies for AI workloads, and capacity forecasting techniques for rapidly evolving model architectures. This presentation addresses the critical reliability challenges that determine whether organizations can successfully operate next-generation AI systems in production environments.",,"Anjan Dash is an accomplished Tech Lead at Meta Platforms Inc., where he specializes in AI/ML infrastructure and privacy-preserving machine learning systems for ad recommendations. With over 15 years of experience in software engineering, he has driven significant business impact, including a 1%+ uplift in ads revenue representing approximately $1 billion in incremental revenue through innovative model architecture and infrastructure improvements. Before Meta, Anjan served as Lead Software Engineer at Amazon Web Services, where he launched the AWS SageMaker Ground Truth service, reducing AI training data labeling costs by up to 70%. His diverse background spans financial services, healthcare technology, and enterprise software, with notable achievements including leading the migration of legacy mainframe systems that generated $3 million in annual savings. Anjan holds a Postgraduate degree in Software Enterprise Management from the Indian Institute of Management, Bangalore, and a Master's in Computer Applications from the National Institute of Technology, Bhopal. He is a recipient of the Scott Cook Innovation Award at Intuit and achieved All India 2nd rank in the MCA entrance examination. Based in Dublin, California, he continues to drive innovation in distributed systems, cloud computing, and AI/ML platforms."
,confirmed,Justin Davis,2,1,Castlight Health,Justin Davis.png,https://www.linkedin.com/in/justin-davis-15565128/,,,,Microfrontend Reliability: SRE Strategies for Distributed Frontend Systems,"Microfrontend architectures with Module Federation introduce distributed system complexity to frontend applications, creating new reliability challenges that traditional SRE practices must adapt to address. This talk explores how to apply Site Reliability Engineering principles to microfrontend systems, ensuring high availability, performance, and operational excellence across independently deployed frontend modules. Unlike backend microservices, microfrontends face unique reliability challenges including runtime integration failures, cross-module dependency conflicts, and cascading failures that can break entire user experiences. We'll examine how to implement the four golden signals for microfrontend systems: measuring latency across module boundaries, tracking traffic patterns for federated modules, monitoring error rates in distributed frontend components, and managing saturation in shared resources and container applications. The session covers essential observability strategies including distributed tracing for microfrontend interactions, real user monitoring (RUM) for federated applications, and synthetic monitoring approaches that validate cross-module integrations. We'll explore how to implement effective circuit breakers for frontend modules, graceful degradation patterns when remote modules fail, and automated rollback strategies for problematic deployments. Attendees will learn production-tested techniques for capacity planning microfrontend systems, including CDN optimization strategies, bundle size monitoring, and performance budget enforcement across teams. The talk addresses critical operational concerns such as coordinated deployments across multiple frontend teams, dependency version management, and incident response procedures for distributed frontend failures. We'll also cover advanced reliability patterns including canary deployments for federated modules, A/B testing infrastructure for microfrontends, and automated performance regression detection. The session explores monitoring strategies for shared design systems, container application health checks, and cross-team SLO management in federated architectures. This session targets SREs, platform engineers, and DevOps professionals working with complex frontend architectures, providing practical frameworks for maintaining reliability, performance, and operational excellence in distributed frontend systems while avoiding common reliability pitfalls.",,"Justin Davis is an innovative Principal Engineer and UI Architect with over 17 years of experience delivering scalable, user-centric digital solutions across healthcare, finance, retail, and media sectors. Currently serving as Principal Engineer at Castlight Health since September 2019, he specializes in full-stack development, modern frontend frameworks, authentication systems, and microservices architecture. At Castlight Health, he has led multiple high-impact projects that enhanced platform capabilities and user experience, including architecting and implementing a multi-tenant OpenID authentication system and leading the migration of a legacy Reporting application to a modern microfrontend architecture using Angular. Prior to his current role, Justin served as UI Architect at Solgenie Technologies from September 2017 to July 2019, where he spearheaded digital transformation initiatives for global clients. His notable achievements include leading the end-to-end development of SAGE, a portfolio management system for BNY Mellon, where he unified multiple legacy systems into a single Angular-based application for portfolio managers overseeing 401k and retirement funds. He also led the migration of DIRECTV's NFL Sunday Ticket platform from Flash/Flex to modern Angular 5, HTML5, and CSS stack, transforming the experience for millions of users. From August 2016 to September 2017, Justin worked as UI Architect and Technical Director at Amazecodes Solutions, leading development projects for Deluxe Corporation and SoulCafe. His responsibilities included creating comprehensive product suites, developing scalable email marketing solutions, and implementing advanced image processing services. During his tenure at Target Corporation from August 2014 to August 2016 as Lead Interactive Specialist, he led UI engineering for the Guest Profile Recommendation Engine and developed a custom jQuery-based product image zoom plugin that replaced the legacy Adobe Scene7 solution. Justin holds a B.Tech in Computer Science Engineering from Calicut University, Kerala, India, completed in May 2007. His technical expertise encompasses JavaScript, Angular, React, Android, iOS, Ionic, Node.js, Java Spring Boot, PHP, Docker, Kubernetes, and Jenkins. He is experienced in DevOps and CI/CD pipelines, cross-functional team leadership, performance optimization, and Agile/Scrum project management, consistently driving projects that align technology strategy with business objectives."
,confirmed,Mike Shi,2,1,ClickHouse,Mike Shi.png,https://www.linkedin.com/in/mikeshi42/,,,,"How We Built ClickStack - an open source, open telemetry native Observability stack","Modern observability is built on a flawed foundation: three siloed pillars - logs, metrics, and traces - each powered by different engines with separate query models, storage formats, and operational costs. Users are forced to manually correlate across systems, accept duplication, or pay high SaaS bills. But what if observability is just a data problem? One that needs a general-purpose solution instead of purpose-built compromises? This talk argues that true observability requires fast, high-cardinality queries over unsampled data at scale and at low cost. Traditional search and metrics engines were not designed for this, but column-oriented databases are. We introduce ClickStack, a fully open source, OpenTelemetry-native observability stack built around ClickHouse. It provides fast, flexible querying and efficient storage, enabling real-time visibility without compromise.",,"Mike Shi is Head of Observability at ClickHouse building ClickStack, the open source observability stack built on ClickHouse and OpenTelemetry. He co-founded HyperDX, acquired by ClickHouse. Mike remains a hands-on developer, contributes to the OpenTelemetry and ClickStack ecosystems, and speaks regularly at observability-focused events."
,confirmed,Mark Freeman,2,1,Gable,Mark Freeman.png,https://www.linkedin.com/in/mafreeman2/,,,,Shadow Dependencies - The Rising Role (Risk?) of Data,"Some of the largest outages on the internet can be traced back not only to changes in code, but also how the code changed underlying data models. Through countless discussions with software engineers, many noted the importance of the underlying data model for quality development, yet also highlighted the lack of incentives (or outright discouragement) by leadership to put in the extra effort to maintain it. Even more troubling, not only are applications impacted by data, but also downstream consumers within the business are taking major dependencies on the output of this data for business-critical workflows-- unbeknownst to the upstream engineers producing the data (i.e., shadow dependencies). In this talk, we highlight this growing problem, why engineer leadership is paying more attention to the risk of data, and how to surface and prevent these issues within the CI/CD workflow via an emerging pattern called ""data contracts.""",,"Mark Freeman, co-author of the upcoming O’Reilly book ""Data Contracts: Developing Production-Grade Pipelines at Scale,"" has spent the past two years collaborating with organizations to implement data contracts and refine best practices. Previously, he was a data scientist turned data engineer with experience in building, putting into production, and maintaining data products across the entire development lifecycle, such as consumer-facing algorithms, personalized insights, and machine learning models. This work has put him at the intersection of software and data teams, where he has become obsessed with the translation between both technical fields."